<div align="center">

# ğŸš€ MLflow Remote Tracking Pipeline

[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)](https://python.org)
[![MLflow](https://img.shields.io/badge/MLflow-Tracking-0194E2?logo=mlflow&logoColor=white)](https://mlflow.org)
[![DVC](https://img.shields.io/badge/DVC-Pipeline-945DD6?logo=dvc&logoColor=white)](https://dvc.org)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-ML-F7931E?logo=scikit-learn&logoColor=white)](https://scikit-learn.org)

**An end-to-end MLOps pipeline for sentiment analysis with automated experiment tracking, model versioning, and registry management.**

</div>

---

## ğŸ“‹ Overview

This project demonstrates a production-ready MLOps workflow using:

- **DVC** for data and pipeline versioning
- **MLflow** for experiment tracking, model logging, and model registry
- **Scikit-learn** for machine learning (BaggingClassifier with DecisionTree)

The pipeline processes text data, engineers Bag-of-Words features, trains a model, evaluates performance, and automatically registers improved models.

---

## ğŸ—ï¸ Project Structure

```
MLflow/
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ raw/              # Original dataset
â”‚   â”œâ”€â”€ interim/          # Train/test split
â”‚   â”œâ”€â”€ processed/        # Cleaned & preprocessed
â”‚   â”œâ”€â”€ features/         # Feature-engineered data
â”‚   â””â”€â”€ external/         # External resources (chat words dict)
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_ingestion.py       # Data loading & splitting
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ data_preprocessing.py   # Text cleaning & processing
â”‚   â”‚   â””â”€â”€ feature_engineering.py  # Bag-of-Words vectorization
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ train_model.py          # Model training
â”‚       â”œâ”€â”€ evaluate_model.py       # Evaluation & MLflow logging
â”‚       â””â”€â”€ register_model.py       # Model registry management
â”‚
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ models/           # Trained model artifacts (.joblib)
â”‚   â””â”€â”€ vectorizers/      # Feature vectorizers (BoW)
â”‚
â”œâ”€â”€ ğŸ“‚ mlartifacts/       # MLflow artifact storage
â”œâ”€â”€ ğŸ“‚ mlruns/            # MLflow run metadata
â”‚
â”œâ”€â”€ ğŸ“„ dvc.yaml           # DVC pipeline definition
â”œâ”€â”€ ğŸ“„ dvc.lock           # DVC pipeline lock file
â”œâ”€â”€ ğŸ“„ params.yaml        # Hyperparameters configuration
â””â”€â”€ ğŸ“„ requirements.txt   # Python dependencies
```

---

## ğŸ”„ Pipeline Stages

The DVC pipeline consists of **6 automated stages**:

```mermaid
graph LR
    A[ğŸ“¥ Data Ingestion] --> B[ğŸ§¹ Preprocessing]
    B --> C[âš™ï¸ Feature Engineering]
    C --> D[ğŸ¯ Model Training]
    D --> E[ğŸ“Š Evaluation]
    E --> F[ğŸ“¦ Registration]
    
    style A fill:#e1f5fe,color:#000
    style B fill:#fff3e0,color:#000
    style C fill:#f3e5f5,color:#000
    style D fill:#e8f5e9,color:#000
    style E fill:#fce4ec,color:#000
    style F fill:#e0f2f1,color:#000
```

| Stage | Script | Description |
|-------|--------|-------------|
| **Data Ingestion** | `data_ingestion.py` | Loads raw data and creates train/test split |
| **Preprocessing** | `data_preprocessing.py` | Cleans text, handles chat abbreviations |
| **Feature Engineering** | `feature_engineering.py` | Generates Bag-of-Words features |
| **Model Training** | `train_model.py` | Trains BaggingClassifier ensemble |
| **Evaluation** | `evaluate_model.py` | Computes metrics & logs to MLflow |
| **Registration** | `register_model.py` | Registers model if performance improves |

---

## âš™ï¸ Configuration

All hyperparameters are centralized in `params.yaml`:

```yaml
data_ingestion:
  seed: 42
  test_size: 0.2

feature_engineering:
  bow_max_features: 1000

model_training:
  estimator:
    criterion: "gini"
    max_depth: 7
    min_samples_split: 5
  bagging:
    n_estimators: 200
    max_samples: 0.85
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv myenv
myenv\Scripts\activate  # Windows
# source myenv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Start MLflow Server

```bash
mlflow server --host 127.0.0.1 --port 8080
```

### 3. Run the Pipeline

```bash
# Execute all stages
dvc repro

# Or run specific stage
dvc repro model_training
```

### 4. View Results

Open [http://127.0.0.1:8080](http://127.0.0.1:8080) in your browser to explore:
- ğŸ“ˆ Experiment metrics and parameters
- ğŸ“Š Confusion matrix and PR curves
- ğŸ“¦ Registered model versions

---

## ğŸ“Š MLflow Tracking Features

| Feature | Description |
|---------|-------------|
| **Parameters** | All hyperparameters from `params.yaml` |
| **Metrics** | Accuracy, F1, Precision, Recall (per class) |
| **Artifacts** | Confusion matrix, PR curves, trained model |
| **Model Registry** | Version control with production aliases |

---

## ğŸ”§ Key Commands

```bash
# Reproduce entire pipeline
dvc repro

# Check pipeline status
dvc status

# Push data to remote storage
dvc push

# View experiment in MLflow UI
mlflow ui --port 8080
```

---

## ğŸ“ Data Flow

```
raw/train.csv
    â†“ [data_ingestion]
interim/{train,test}.csv
    â†“ [data_preprocessing]
processed/{train,test}.csv
    â†“ [feature_engineering]
features/{train,test}.csv + vectorizers/bow.joblib
    â†“ [model_training]
models/bagging_classifier.joblib
    â†“ [evaluate â†’ register]
MLflow Model Registry
```

---

## ğŸ“ License

This project is licensed under the terms specified in the [LICENSE](LICENSE) file.

---

<div align="center">

**Made with â¤ï¸ for MLOps Learning**

</div>